{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install schedule"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLzs4Tm_OynP",
        "outputId": "a405c48a-7c43-4504-dcd2-b31e0116d90b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import requests\n",
        "import urllib.parse\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import os\n",
        "import html\n",
        "import schedule #install 필수\n",
        "import time\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "boj0HNckQYoI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ID,Secret은  https://developers.naver.com/apps/#/list에서 API를 등록 후 ID,Secret를 아래 Id,secret에 넣으시면 됩니다.\n",
        "Id= 'Your_ID'\n",
        "secret= 'Your_secret'\n",
        "\n",
        "\n",
        "CSV_FILE_PATH = 'naver_news_data.csv'  #csv파일 이름\n",
        "\n",
        "\n",
        "#이전에 API 사용 없이는 수백개 데이터를 가져 올 수 있었지만 현재는 회 당 최대 100 일당 최대 20000으로 제한이 걸렸습니다.\n",
        "#그래서 스캐줄 또는 코드를 통해서 시간마다 자동으로 저장을 하여 데이터를 가져오거나 일일이 작동 시켜야합니다.\n",
        "\n",
        "\n",
        "def url_make(keyword, display=10, start=1, sort='sim'):\n",
        "    '''\n",
        "    Keyword: 키워드\n",
        "\n",
        "    display 출력개수 min:1 max:100\n",
        "\n",
        "    start 출력지점 min:1 max:1000\n",
        "\n",
        "    sort 정렬기준 sim(정확도) date(최신순)\n",
        "\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    # 각 정보를 담을 빈 리스트 초기화\n",
        "    titles = []\n",
        "    describes = []\n",
        "    datetimes = []\n",
        "    links = []\n",
        "    encText = urllib.parse.quote(keyword)\n",
        "    # URL 생성 (display 파라미터 위치 수정)\n",
        "    url = f\"https://openapi.naver.com/v1/search/news.xml?query={encText}&display={display}&start={start}&sort={sort}\"\n",
        "    request = urllib.request.Request(url)\n",
        "    request.add_header(\"X-Naver-Client-Id\", Id)\n",
        "    request.add_header(\"X-Naver-Client-Secret\", secret)\n",
        "\n",
        "    response = urllib.request.urlopen(request)\n",
        "    rescode = response.getcode()\n",
        "\n",
        "    if rescode == 200:\n",
        "        response_body = response.read()\n",
        "        xml_string = response_body.decode('utf-8')\n",
        "        root = ET.fromstring(xml_string)\n",
        "\n",
        "        for item in root.findall('./channel/item'): # 루프 한 번만 실행\n",
        "            # 각 요소 찾기\n",
        "            title_element = item.find('title')\n",
        "            link_element = item.find('link')\n",
        "            describe_element = item.find('description')\n",
        "            date_element = item.find('pubDate')\n",
        "\n",
        "            # Title 처리\n",
        "            if title_element is not None and title_element.text is not None:\n",
        "                raw_title = title_element.text\n",
        "                # HTML 태그 제거 및 HTML 엔티티 변환 (&quot; 등)\n",
        "                clean_title = html.unescape(raw_title.replace('<b>', '').replace('</b>', '')).replace('&quot;', '\"')\n",
        "                titles.append(clean_title)\n",
        "            else:\n",
        "                titles.append(None)\n",
        "\n",
        "            # Link 처리\n",
        "            if link_element is not None and link_element.text is not None:\n",
        "                links.append(link_element.text)\n",
        "            else:\n",
        "                links.append(None)\n",
        "\n",
        "            # Description 처리\n",
        "            if describe_element is not None and describe_element.text is not None:\n",
        "                raw_describe = describe_element.text\n",
        "                clean_describe = html.unescape(raw_describe.replace('<b>', '').replace('</b>', ''))\n",
        "                describes.append(clean_describe)\n",
        "            else:\n",
        "                describes.append(None)\n",
        "\n",
        "            # pubDate 처리\n",
        "            if date_element is not None and date_element.text is not None:\n",
        "                datetimes.append(date_element.text) # 날짜 형식 변환은 필요시 추가\n",
        "            else:\n",
        "                datetimes.append(None)\n",
        "\n",
        "    else:\n",
        "        print(f\"Error Code: {rescode}\")\n",
        "        # API 오류 시 빈 DataFrame 반환 또는 예외 발생 등 처리 가능\n",
        "        return pd.DataFrame() # 빈 DataFrame 반환 예시\n",
        "\n",
        "    news_df = pd.DataFrame({\n",
        "        'Title': titles,\n",
        "        'Describe': describes,\n",
        "        'Link': links,\n",
        "        'DateTime': datetimes\n",
        "    })\n",
        "\n",
        "    if not os.path.exists(CSV_FILE_PATH):\n",
        "        news_df.to_csv(CSV_FILE_PATH, index=False, mode='w', encoding='utf-8-sig')\n",
        "    else:\n",
        "        news_df.to_csv(CSV_FILE_PATH, index=False, mode='a', header=False, encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "    # 생성된 DataFrame 반환\n",
        "    return news_df"
      ],
      "metadata": {
        "id": "URkQfY24AC1a"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def job():\n",
        "    print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] 작업을 실행합니다: 키워드='대선'\")\n",
        "    try:\n",
        "        url_make('대선', display=100,sort='date') # 원하는 값 설정 가능 (범위는 위에서 제공)\n",
        "    except Exception as e:\n",
        "        print(f\"작업 실행 중 오류 발생: {e}\")\n",
        "\n",
        "# 매 30분마다 job 함수 실행 예약\n",
        "schedule.every(30).minutes.do(job)\n",
        "\n",
        "#최소 1회 실행\n",
        "job()\n",
        "\n",
        "# 예약된 작업들을 계속 확인하고 실행하는 루프\n",
        "start_time = time.time()\n",
        "run_seconds = 48 * 60 * 60 # 48시간\n",
        "\n",
        "while time.time() - start_time < run_seconds:\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)\n",
        "\n",
        "schedule.clear()\n",
        "print(\"수집 종료.\")"
      ],
      "metadata": {
        "id": "uG0p5ttnMqjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_naver_news_body(url):\n",
        "    \"\"\"\n",
        "    네이버 뉴스 URL을 받아 본문 텍스트를 추출하는 함수\n",
        "\n",
        "    Args:\n",
        "        url (str): 네이버 뉴스 기사 URL (mnews.naver.com 형태)\n",
        "\n",
        "    Returns:\n",
        "        str: 추출된 기사 본문 텍스트. 실패 시 None 반환.\n",
        "    \"\"\"\n",
        "    if not url or not url.startswith(\"https://n.news.naver.com/mnews/article/\"):\n",
        "        print(\"올바른 네이버 뉴스 기사 URL 형식이 아닙니다.\")\n",
        "        return None\n",
        "\n",
        "    headers = {\n",
        "        # 웹사이트가 봇으로 인식하여 차단하는 것을 피하기 위해 일반적인 브라우저 User-Agent 설정\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "\n",
        "    # URL에 HTTP GET 요청 보내기\n",
        "    response = requests.get(url, headers=headers, timeout=10)\n",
        "    # HTTP 상태 코드가 200 (성공) 인지 확인\n",
        "    response.raise_for_status() # 200이 아니면 HTTPError 발생\n",
        "\n",
        "    # 응답받은 HTML 내용을 BeautifulSoup으로 파싱\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    #본문 위치 찾기\n",
        "    article_body_tag = soup.find('article', id='dic_area')\n",
        "\n",
        "    if article_body_tag:\n",
        "        # 불필요한 태그 제거 (예: 기자 정보, 광고 등) - 선택 사항\n",
        "        # 예시: 기사 본문 내 '기자 프로필' 부분 제거 (실제 구조에 따라 수정 필요)\n",
        "        unwanted_elements = article_body_tag.find_all('div', attrs={'class':'journalistcard_module'})\n",
        "        for element in unwanted_elements:\n",
        "            element.decompose() # 태그 제거\n",
        "\n",
        "        # 이미지 설명이나 캡션 등 특정 부분 제외가 필요하다면 추가 로직 구현 -선택사항\n",
        "        unwanted_spans = article_body_tag.find_all('span', attrs={'class':'end_photo_org'})\n",
        "        for span in unwanted_spans:\n",
        "            span.decompose()\n",
        "\n",
        "        # 태그 내부의 텍스트만 추출\n",
        "        # separator='\\n' : 각 텍스트 조각 사이에 줄바꿈 추가\n",
        "        # strip=True : 각 텍스트 조각의 앞뒤 공백 제거\n",
        "        body_text = article_body_tag.get_text(separator='\\n', strip=True)\n",
        "        return body_text"
      ],
      "metadata": {
        "id": "yRRCRu3jNcMY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_data=pd.read_csv('naver_news_data.csv')\n",
        "link=news_data['Link']\n",
        "contents = []\n",
        "for i in link:\n",
        "    content=get_naver_news_body(i)\n",
        "    contents.append(content)\n",
        "\n",
        "news_data['Contents'] = contents"
      ],
      "metadata": {
        "id": "gbnpJktdOYnw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}